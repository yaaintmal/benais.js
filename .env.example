# Ollama API Configuration
OLLAMA_URL=http://localhost:11434/api/chat # point to your dedicated ollama-instance

# LLM Model Configuration
LLM_MODEL=gemma3 # choose a light model for faster response

# Optional Settings
DEBUG=false
